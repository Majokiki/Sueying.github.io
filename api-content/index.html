{"posts":[{"title":"python Mysql 基础使用","content":"1. 创建数据库 import mysql.connector mydb = mysql.connector.connect( host=&quot;localhost&quot;, user=&quot;yourusername&quot;, passwd=&quot;yourpassword&quot; ) mycursor = mydb.cursor() mycursor.execute(&quot;CREATE DATABASE mydatabase&quot;) mycursor.execute(&quot;SHOW DATABASES&quot;) for x in mycursor:print(x) 2. 创建表，查看表，删除表 #先连接创建好的数据库 mydb = mysql.connector.connect( host=&quot;localhost&quot;, user=&quot;yourusername&quot;, passwd=&quot;yourpassword&quot;, database=&quot;mydatabase&quot; ) mycursor = mydb.cursor() # 建立顾客表，主键是自增的整型id，其他列是：不定长字符串name，不定长字符串address mycursor.execute(&quot;CREATE TABLE customers (name VARCHAR(255), address VARCHAR(255))&quot;) mycursor.execute(&quot;ALTER TABLE customers ADD COLUMN id INT AUTO_INCREMENT PRIMARY KEY&quot;) # 建立消费记录表，主键是唯一且随机的编码visitNo，result列保存json格式的记录 mycursor.execute(&quot;CREATE TABLE records (visitNo VARCHAR(32) PRIMARY KEY, result TEXT）&quot;) mycursor.execute(&quot;SHOW TABLES&quot;) for x in mycursor:print(x) mycursor.execute(&quot;DROP TABLE IF EXISTS customers&quot;） 3. 插入数据，查找数据，删除数据，更新数据 mycursor.execute(&quot;INSERT INTO customers (name, address) VALUES (%s, %s)&quot; % (&quot;John&quot;, &quot;Highway 21&quot;)) mycursor.execute(&quot;SELECT * FROM customers WHERE address ='Park Lane 38'&quot;） myresult = mycursor.fetchall() for x in myresult : print(x) mycursor.execute(&quot;SELECT * FROM customers WHERE address LIKE '%way%'&quot;） myresult = mycursor.fetchall() for x in myresult : print(x) mycursor.execute(&quot;SELECT * FROM customers ORDER BY name DESC&quot;) myresult = mycursor.fetchall() for x in myresult : print(x) mycursor.execute( &quot;DELETE FROM customers WHERE address = 'Mountain 21'&quot; ) mydb.commit() print(mycursor.rowcount, &quot;record(s) deleted&quot;) mycursor.execute(&quot;UPDATE customers SET address = 'Canyon 123' WHERE address = 'Valley 345'&quot;) mydb.commit() print(mycursor.rowcount, &quot;record(s) affected&quot;) 特别地，插入json格式数据时： # type(tmp) = &lt;class 'dict'&gt; # result = json.dumps(tmp) # result格式为json str # 若result中含有Unicode中文字符，需要转义json数据：result.replace('\\\\u','\\\\\\\\u') # mycursor.execute(&quot;select left(uuid(),8)&quot;) # for x in mycursor : visitNo = str(x[0]) # uuid截取后8位作为唯一且随机的编码visitNo mycursor.execute(&quot;insert into records (visitNo, result) values ('%s', '%s')&quot; % (visitNo, result)) # 查找记录 cursor.execute(&quot;select result from records where visitNo = '%s'&quot; % (visitNo)) ","link":"https://sueying.github.io/post/python-mysql/"},{"title":"SNN-DPC 聚类算法","content":"DPC聚类算法 2014年，Rodriguez等人在Science杂志发表了基于快速搜索和发现密度峰值的聚类算法（clustering by fast search and find of density peaks, DPC)。该算法可以自动地发现簇中心，然后将数据点分配到不同的簇中。 该算法基于两个基本假设：1）簇中心（密度峰值点）的局部密度大于围绕它的邻居的局部密度；2）不同簇中心之间的距离相对较远。为了找到同时满足这两个条件的簇中心，该算法引入了局部密度的定义。 1. 局部密度 数据集dataset中的数据点 iii 具有局部密度性质ρi\\rho_{i}ρi​ 基于截断距离的局部密度定义：ρi=∑i≠jχ(dij−dc),χ(x)={1,x&lt;00,x≥0\\rho_{i}=\\sum_{i \\neq j} \\chi\\left(d_{i j}-d_{c}\\right), \\chi(x)=\\left\\{\\begin{array}{ll}1, &amp; x&lt;0 \\\\ 0, &amp; x \\geq 0\\end{array}\\right.ρi​=∑i​=j​χ(dij​−dc​),χ(x)={1,0,​x&lt;0x≥0​ 基于核距离的局部密度定义：ρi=∑i≠jexp⁡[−(dijdc)2]\\rho_{i}=\\sum_{i \\neq j} \\exp \\left[-\\left(\\frac{d_{i j}}{d_{c}}\\right)^{2}\\right]ρi​=∑i​=j​exp[−(dc​dij​​)2] 其中：dijd_{i j}dij​是数据点 i 和 j 的欧几里得距离； dc&gt;0d_{c}&gt;0dc​&gt;0，是自定义的截断距离。即局部区域的定义是以点 i 为圆心，dcd_{c}dc​为半径的圆形区域。 这两种局部密度定义的主要区别是，二者分别是离散的和连续的。 2. 到更高密度点的最近距离 δi=min⁡j:ρj&gt;ρi(dij)\\delta_{i}=\\min _{j: \\rho_{j}&gt;\\rho_{i}}\\left(d_{i j}\\right)δi​=minj:ρj​&gt;ρi​​(dij​) 特殊地，对于局部密度峰值点iii，定义δi=max⁡i≠j(δj)\\delta_{i}=\\max _{i \\neq j}\\left(\\delta_{j}\\right)δi​=maxi​=j​(δj​) 3. 决策值 为了方便地选择簇中心，取ρ\\rhoρ和δ\\deltaδ的乘积作为决策值加以衡量。 γi=ρi×δi\\gamma_{i}=\\rho_{i} \\times \\delta_{i}γi​=ρi​×δi​ 取决策值γ\\gammaγ最大的点作为簇中心。 DPC步骤 计算任意两个数据点之间的距离Dn×n={dij}n×nD^{n \\times n}=\\left\\{d_{i j}\\right\\}^{n \\times n}Dn×n={dij​}n×n 根据截断距离算出任意数据点 xix_{i}xi​ 的局部密度 ρi\\rho_{i}ρi​ 对于任意数据点 xi,x_{i},xi​, 计算出 δi\\delta_{i}δi​ 以 ρi\\rho_{i}ρi​ 为橫轴，以 δi\\delta_{i}δi​ 为纵轴，画出决策图 利用决策图，将 ρi\\rho_{i}ρi​ 和 δi\\delta_{i}δi​ 都相对较高的点标记为簇中心; 将 ρi\\rho_{i}ρi​ 相对较低但是 δi\\delta_{i}δi​ 相对较高的点标记为噪声点 将剩余点进行分配。分配时，将每个剩余点分配到它的最近邻且密度比其大的数据点所在的簇。 SNN-DPC算法 Rui.Liu等人发现，当两个簇的密度差异较大时（如经典的聚类数据集Jain），DPC算法选择的两个簇中心往往只位于密度较大簇上。通过改变局部密度的定义（基于共享k近邻点SNN（Shared Nearest Neighbors）的个数计算），来改进效果。 定义Shared Nearest Neighbors 对于数据集中的任意点 i 和 j ，Γ(i)\\Gamma(i)Γ(i)是点 i 的 K个近邻点集合，Γ(j)\\Gamma(j)Γ(j)是点 j 的 K个近邻点集合。 点 i 和点 j 的共享近邻点：SNN(i,j)=Γ(i)∩Γ(j)SNN(i, j)=\\Gamma(i) \\cap \\Gamma(j)SNN(i,j)=Γ(i)∩Γ(j) 定义SNN Similarity sim⁡(i,j)={∣SNN(i,j)∣2∑p∈SNN(i,j)(dip+djp), if i,j∈SNN(i,j)0, otherwise \\operatorname{sim}(i, j)=\\left\\{\\begin{array}{cl}\\frac{|S N N(i, j)|^{2}}{\\sum_{p \\in SNN(i, j)}\\left(d_{ip}+d_{jp}\\right)} , &amp; \\text { if } i, j \\in SNN(i, j) \\\\ 0, &amp; \\text { otherwise }\\end{array}\\right.sim(i,j)={∑p∈SNN(i,j)​(dip​+djp​)∣SNN(i,j)∣2​,0,​ if i,j∈SNN(i,j) otherwise ​ 原式可化为∣SNN(i,j)∣×11∣SNN(i,j)∣∑p∈SNN(i,j)(dip+djp)|SNN(i, j)| \\times \\frac{1}{\\frac{1}{|S N N(i, j)|} \\sum_{p \\in S N N(i, j)}\\left(d_{i p}+d_{j p}\\right)}∣SNN(i,j)∣×∣SNN(i,j)∣1​∑p∈SNN(i,j)​(dip​+djp​)1​， 其中： ∣SNN⁡(i,j)∣|\\operatorname{SNN}(i, j)|∣SNN(i,j)∣的物理含义是点 i 和点 j 共享近邻点的个数 1∣SNN(i,j)∣∑p∈SNN(i,j)(dip+djp)\\frac{1}{|S N N(i, j)|} \\sum_{p \\in S N N(i, j)}\\left(d_{i p}+d_{j p}\\right)∣SNN(i,j)∣1​∑p∈SNN(i,j)​(dip​+djp​)的物理含义是点 i 和点 j 到他们的所有共享近邻点的距离的平均值 定义SNN局部密度 定义点 i 为数据集中的任意一点，L(i)={x1,x2,…,xk}L(i)=\\left\\{x_{1}, x_{2}, \\ldots, x_{k}\\right\\}L(i)={x1​,x2​,…,xk​}是与点 i 的SNN Similarity最高的 k 个点。 SNN局部定义为：ρi=∑j∈L(i)sim⁡(i,j)\\rho_{i}=\\sum_{j \\in L(i)} \\operatorname{sim}(i, j)ρi​=∑j∈L(i)​sim(i,j) 在这种定义下，局部密度不仅包含了距离信息，而且还通过许多SNN，包含了集群结构的信息 定义到更高密度点的最近距离 δi=min⁡j:ρj&gt;ρi[dij(∑p∈Γ(i)dip+∑q∈Γ(j)djq)]\\delta_{i}=\\min _{j: \\rho_{j}&gt;\\rho_{i}}\\left[d_{i j}\\left(\\sum_{p \\in \\Gamma(i)} d_{i p}+\\sum_{q \\in \\Gamma(j)} d_{j q}\\right)\\right]δi​=minj:ρj​&gt;ρi​​[dij​(∑p∈Γ(i)​dip​+∑q∈Γ(j)​djq​)]，注意这里的dijd_{i j}dij​和(∑p∈Γ(i)dip+∑q∈Γ(j)djq)\\left(\\sum_{p \\in \\Gamma(i)} d_{i p}+\\sum_{q \\in \\Gamma(j)} d_{j q}\\right)(∑p∈Γ(i)​dip​+∑q∈Γ(j)​djq​)是相乘的关系，因为我们定义局部密度时，使用的距离定义有所改变。 特别的，局部密度最大值点的δi=max⁡j∈(X−i)(δj)\\delta_{i}=\\max _{j \\in(X-i)}\\left(\\delta_{j}\\right)δi​=maxj∈(X−i)​(δj​) 定义决策值 类似的，决策值γi=ρi×δi\\gamma_{i}=\\rho_{i} \\times \\delta_{i}γi​=ρi​×δi​ 定义剩余点分配步骤中的绝对下属点和可能下属点 假设点 A 已经被分配到某个簇 c 中，点 B 尚未被分配： 若∣SNN(A,B)∣≥k/2|S N N(A, B)| \\geq k / 2∣SNN(A,B)∣≥k/2，则称点 B 是簇 c 的绝对下属点，也就是说点 B 也一定会被分配到簇 c 中。 若0&lt;∣SNN(A,B)∣&lt;k/20&lt;|S N N(A, B)|&lt;k / 20&lt;∣SNN(A,B)∣&lt;k/2，则称点 B 是簇 c 的可能下属点，点 B 可以是其他簇的下属点。 在极端情况下，假设点 B 同时满足 ∣SNN(A1,B)∣=k/2|S N N(A_1, B)| = k / 2∣SNN(A1​,B)∣=k/2 ∣SNN(A2,B)∣=k/2|S N N(A_2, B)| = k / 2∣SNN(A2​,B)∣=k/2，可以暂时将 k 加一，使其成为奇数 SNN-DPC步骤 计算任意两个数据点之间的距离Dn×n={dij}n×nD^{n \\times n}=\\left\\{d_{i j}\\right\\}^{n \\times n}Dn×n={dij​}n×n 根据定义计算任意数据点的 SNN Similarity 算出任意数据点 xix_{i}xi​ 的局部密度 ρi\\rho_{i}ρi​ 对于任意数据点 xi,x_{i},xi​, 计算出 δi\\delta_{i}δi​ 以 ρi\\rho_{i}ρi​ 为橫轴，以 δi\\delta_{i}δi​ 为纵轴，画出决策图。或者以 iii 为橫轴，以 γi\\gamma_{i}γi​ 为纵轴画出决策图 利用决策图，将 ρi\\rho_{i}ρi​ 和 δi\\delta_{i}δi​ 都相对较高的点标记为簇中心; 或者将γi\\gamma_{i}γi​较大的点作为簇中心 将剩余点中的绝对下属点进行分配。 将剩余点中的可能下属点进行分配。 ","link":"https://sueying.github.io/post/snn-dpc/"},{"title":"CheatSheet系列","content":"许多细碎的东西不容易记住，但可以放进我们的cache里。这个系列是常用工具的CheatSheet，只包含各个工具最主要的功能。持续更新中... Pandas Cheet Sheet ","link":"https://sueying.github.io/post/cheatsheets/"},{"title":"ACM中哪些算法是应该敲的滚瓜烂熟的？","content":"ACM中哪些算法是应该敲的滚瓜烂熟的？ - 蔡少斐的回答 - 知乎 https://www.zhihu.com/question/68701377/answer/268045983 数据结构 线段树（维护区间操作） 二叉搜索树 树状数组（维护前缀操作，某些情况下变体也可以维护区间） 单调栈（在需要栈性质的前提下维护单调序列） 单调队列（维护单调序列，常用于动态规划的优化、斜率优化等等） 并查集（判断连通性，传递逻辑关系） 带权并查集（常用于节点之间有可用权值表示的逻辑关系问题的求解） 哈希表（手写的好处是方便自己实现对任何形式的数据哈希映射） trie树（涉及到二进制数相关问题，还是AC自动机的基础） treap平衡树（维护有序序列，对其进行二分查找，求k大，求前驱后继等操作） 伸展树（维护区间，支持对区间的反转，区间插入等操作，还可以当作平衡树使用）。 图的存储方法 邻接表（链式前向星，vector） 邻接矩阵。 背包问题 01背包 完全背包 多重背包 分组背包 树上分组背包。 动态规划 线性动态规划 区间动态规划 树上动态规划 图上动态规划 期望动态规划 概率动态规划 数位动态规划 状态压缩动态规划。 动态规划优化 利用常见数据结构（单调栈、单调队列、线段树、ST表等等） 四边形优化（对于满足四边形不等式的转移方程形式） 斜率优化（对于满足一次函数的转移方程） FFT优化（对于满足卷积形式的转移方程） CDQ分治优化。 排序方法 堆排序 快速排序 归并排序 插入排序 选择排序 冒泡排序 计数排序 基数排序（后缀数组的基础，重要）。 基本技巧 枚举子集 莫队离线 启发式合并。 图论算法 tarjan（求解强联通分量、双联通分量、割点、桥、以及环相关问题） dijkstra（求解单源最短路，优化特定形式动态规划问题） kruscal（最小生成树、最大生成树） spfa（单源最长/最短路，判定负环，费用流算法的基础，求解差分约束系统） floyd（求解多源最短路，解带限制的最短路问题，求最小环） 拓扑排序（求解DAG上带限制的问题，解决动态规划状态依赖关系） 欧拉路径（求解欧拉回路等，应用不多） 匈牙利算法（二分图匹配） KM算法（最大权完美匹配）。 字符串算法 kmp（用于单模式串匹配任务，也可用于求字符串的周期） AC自动机（多模式串匹配有关的字符串任务，现多与动态规划及图论问题同时出现） manacher算法（用于求解回文串相关的题目） 后缀数组（字符串子串排序有关） 字符串哈希（极为常用的算法，用于判断字符串相等）。 搜索算法 bfs（求解最小步数问题） dfs（非常基本） 迭代加深搜索（也是求解最小步数问题，具体可以使用迭代+dfs实现） A*（启发式搜索，加速搜索过程） 双向广搜（起始状态与终止状态均一致，根号级别加速搜索） 折半搜索（根号级别加速搜索） 模拟退火（全局寻找最优解，用处非常广泛的算法） 二分搜索（非常常用，二分答案问题) 三分搜索（求解凸函数极值） ","link":"https://sueying.github.io/post/acm-zhong-na-xie-suan-fa-shi-ying-gai-qiao-de-gun-gua-lan-shou-de/"},{"title":"新词发现","content":"怎样才算一个词？ term在文章中出现的次数够多 词语的内部凝聚程度要足够高 以“知”、“乎” 这两个字来说，假设在 5000 万字的样本中, “知” 出现了 150 万次， “乎” 出现了 4 万次。那 “知” 出现的概率为 0.03, “乎” 出现的概率为 0.0008。如果两个字符出现是个独立事件的话，”知”、“乎” 一起出现的期望概率是 0.03 * 0.0008 = 2.4e-05. 如果实际上 “知乎” 出现了 3 万次, 则实际上”知”、“乎” 一起出现的概率是 6e-03, 是期望概率的 250 倍。也就是说两个字越相关，点间互信息越大。 可以用点间互信息衡量词语凝固度： PMI=p(x,y)log⁡2p(x,y)p(x)p(y)P M I=p(x, y) \\log _{2} \\frac{p(x, y)}{p(x) p(y)} PMI=p(x,y)log2​p(x)p(y)p(x,y)​ 光看文本片段内部的凝合程度还不够，我们还需要从整体来看它在外部的表现。考虑“被子”和“辈子”这两个片段。我们可以说“买被子”、“盖被子”、“进被子”、“好被子”、“这被子”等等，在“被子”前面加各种字；但“辈子”的用法却非常固定，除了“一辈子”、“这辈子”、“上辈子”、“下辈子”，基本上“辈子”前面不能加别的字了。“辈子”这个文本片段左边可以出现的字太有限，以至于直觉上我们可能会认为，“辈子”并不单独成词，真正成词的其实是“一辈子”、“这辈子”之类的整体。可见，文本片段的自由运用程度也是判断它是否成词的重要标准。如果一个文本片段能够算作一个词的话，它应该能够灵活地出现在各种不同的环境中，具有非常丰富的左邻字集合和右邻字集合。 词语的左右邻字要足够丰富--自由程度 如果一个字符组合可以成词，它应当出现在丰富的语境中，也就是说，拥有丰富的左右邻字。 信息熵是对信息量多少的度量，信息熵越高，表示信息量越丰富、不确定性越大。因此字符组合左右邻字的丰富程度，可以用信息熵(Entropy)来表示： Entropy⁡(w)=−∑wn∈WNeighborP(wn∣w)log⁡2P(wn∣w)\\operatorname{Entropy}(w)=-\\sum_{w_{n} \\in W_{\\text {Neighbor}}} P\\left(w_{n} \\mid w\\right) \\log _{2} P\\left(w_{n} \\mid w\\right) Entropy(w)=−wn​∈WNeighbor​∑​P(wn​∣w)log2​P(wn​∣w) 综合考虑 在实际运用中你会发现，文本片段的凝固程度和自由程度，两种判断标准缺一不可。只看凝固程度的话，程序会找出“巧克”、“俄罗”、“颜六色”、“柴可夫”等实际上是“半个词”的片段；只看自由程度的话，程序则会把“吃了一顿”、“看了一遍”、“睡了一晚”、“去了一趟”中的“了一”提取出来，因为它的左右邻字都太丰富了。 实现流程 生成候选词：将文本按字符分割后拼接为候选词 把文本按标点切分成句，然后提取每句话的2元组,3元组,...,k元组作为候选词(一般k&lt;=5就够用了)。比如“专注于自然语言处理技术”这句话产生的2元候选词有：[“专注”、“注于”、“于自”、“自然”、“然语”、“语言”、“言处”、“处理”、“理技”、“技术”]。 将字符两两组合作为候选词 候选词得分计算 我们为每个候选词计算一个得分，表示它成词的可能性。 为了度量左右邻字丰富程度，我们综合考虑候选词左右信息熵(LE,RE)(LE,RE)(LE,RE)的大小、LE与RE差的绝对值(∣LE−RE∣|LE-RE|∣LE−RE∣)，构造统计量如下： L(W)=log⁡LE⋅eRE+RE⋅eLE∣LE−RE∣L(W)=\\log \\frac{L E \\cdot e^{R E}+R E \\cdot e^{L E}}{|L E-R E|} L(W)=log∣LE−RE∣LE⋅eRE+RE⋅eLE​ 由于点间互信息的值会受到候选词长度的影响（候选词越长，互信息取值偏大），我们使用平均互信息(AMI)作为词语内聚程度的度量。AMI的公式如下： AMI=1nlog⁡p(W)p(c1)⋯p(cn)A M I=\\frac{1}{n} \\log \\frac{p(W)}{p\\left(c_{1}\\right) \\cdots p\\left(c_{n}\\right)} AMI=n1​logp(c1​)⋯p(cn​)p(W)​ 候选词得分： score=AMI+L(W)\\text {score}=A M I+L(W) score=AMI+L(W) ","link":"https://sueying.github.io/post/xin-ci-fa-xian/"},{"title":"Pandas cheatsheet","content":"pandas 时间序列 之日期范围、频率及移动 https://blog.csdn.net/weixin_38656890/article/details/79782188 ","link":"https://sueying.github.io/post/pandas-cheatsheet/"},{"title":"小工具","content":"nohup nohup python finalweb.py --port=54345 &amp; jobs kill %1 ","link":"https://sueying.github.io/post/easy-tools/"},{"title":"mac test","content":"mac test ","link":"https://sueying.github.io/post/mac_test/"},{"title":"五分钟实现一个中文版的Bert Masked LM","content":"试用一下超好用的HuggingFace Transformer，用它实现一个中文版的Bert Masked LM ","link":"https://sueying.github.io/post/easy-bert-maskLM/"},{"title":"MarkDown表格格式编辑","content":"只需要贴一段css在正文中就可以，这段css代码会隐藏，不出现在显示效果中。 &lt;style&gt; table { width: 100%; /*表格宽度*/ border-collapse: collapse; /*使用单一线条的边框*/ empty-cells: show; /*单元格无内容依旧绘制边框*/ } table th,td { height: 35px; /*统一每一行的默认高度*/ } table th { font-weight: bold; /*加粗*/ text-align: center !important; /*内容居中，加上 !important 避免被 Markdown 样式覆盖*/ background: #ECF2F9; /*背景色*/ white-space: nowrap; /*表头内容强制在一行显示*/ } /* 隔行变色 */ table tbody tr:nth-child(2n) { background: #F4F7FB; } /* 悬浮变色 */ table tr:hover { background: #B2B2B2; } /* 首列不换行 */ table td:nth-child(1) { white-space: nowrap; } /* 指定列宽度 */ table th:nth-of-type(2) { width: 40%; white-space: nowrap; } table th:nth-of-type(3) { width: 40%; white-space: nowrap; } &lt;/style&gt; ","link":"https://sueying.github.io/post/edit-markdown-table-style/"},{"title":"Review Leetcode算法题","content":"由于算法题通常会刷不止一遍，除了按专题整理算法题之外，我觉得还应该有一个表格来记录：题目描述，解题思路，以及刷题的记录。 table { width: 100%; /*表格宽度*/ border-collapse: collapse; /*使用单一线条的边框*/ empty-cells: show; /*单元格无内容依旧绘制边框*/ } table th,td { height: 35px; /*统一每一行的默认高度*/ } table th { font-weight: bold; /*加粗*/ text-align: center !important; /*内容居中，加上 !important 避免被 Markdown 样式覆盖*/ background: #ECF2F9; /*背景色*/ white-space: nowrap; /*表头内容强制在一行显示*/ } /* 隔行变色 */ table tbody tr:nth-child(2n) { background: #F4F7FB; } /* 悬浮变色 table tr:hover { background: #B2B2B2; }*/ /* 首列不换行 */ table td:nth-child(1) { white-space: nowrap; } /* 指定列宽度 */ table th:nth-of-type(2) { width: 40%; white-space: nowrap; } table th:nth-of-type(3) { width: 40%; white-space: nowrap; } 类型：二叉搜索树 描述 思路 records 判断是否是二叉搜索树 类型：数组 描述 思路 records 152.乘积最大的子数组 给你一个整数数组 nums ，请你找出数组中乘积最大的连续子数组（该子数组中至少包含一个数字），并返回该子数组所对应的乘积。 相当于求被 0 拆分的各个子数组的最大值；数组中没有 0 时讨论负数的个数奇偶：偶数个时直接返回所有元素的乘积，奇数个时比较左边的最大乘积和右边的最大乘积。 a ","link":"https://sueying.github.io/post/review-leetcode/"},{"title":"MySQL cheatsheet","content":"一些 MySQL基本操作，帮助回忆MySQL语法 转载自 Brad Traversy: MySQL cheatsheet Brad Traversy的MySQL视频 MySQL Locations Mac /usr/local/mysql/bin Windows /Program Files/MySQL/MySQL version/bin Xampp /xampp/mysql/bin Add mysql to your PATH # 在任何working directory下都能打开 mysql # Current Session export PATH=${PATH}:/usr/local/mysql/bin # Permanantly echo 'export PATH=&quot;/usr/local/mysql/bin:$PATH&quot;' &gt;&gt; ~/.bash_profile On Windows - https://www.qualitestgroup.com/resources/knowledge-center/how-to-guide/add-mysql-path-windows/ Login mysql -u root -p Show Users SELECT User, Host FROM mysql.user; Create User CREATE USER 'someuser'@'localhost' IDENTIFIED BY 'somepassword'; Grant All Priveleges On All Databases GRANT ALL PRIVILEGES ON * . * TO 'someuser'@'localhost'; FLUSH PRIVILEGES; Show Grants SHOW GRANTS FOR 'someuser'@'localhost'; Remove Grants REVOKE ALL PRIVILEGES, GRANT OPTION FROM 'someuser'@'localhost'; Delete User DROP USER 'someuser'@'localhost'; Exit exit; Show Databases SHOW DATABASES Create Database CREATE DATABASE acme; Delete Database DROP DATABASE acme; Select Database USE acme; Create Table CREATE TABLE users( id INT AUTO_INCREMENT, first_name VARCHAR(100), last_name VARCHAR(100), email VARCHAR(50), password VARCHAR(20), location VARCHAR(100), dept VARCHAR(100), is_admin TINYINT(1), register_date DATETIME, PRIMARY KEY(id) ); Delete / Drop Table DROP TABLE tablename; Show Tables SHOW TABLES; Insert Row / Record INSERT INTO users (first_name, last_name, email, password, location, dept, is_admin, register_date) values ('Brad', 'Traversy', 'brad@gmail.com', '123456','Massachusetts', 'development', 1, now()); Insert Multiple Rows INSERT INTO users (first_name, last_name, email, password, location, dept, is_admin, register_date) values ('Fred', 'Smith', 'fred@gmail.com', '123456', 'New York', 'design', 0, now()), ('Sara', 'Watson', 'sara@gmail.com', '123456', 'New York', 'design', 0, now()),('Will', 'Jackson', 'will@yahoo.com', '123456', 'Rhode Island', 'development', 1, now()),('Paula', 'Johnson', 'paula@yahoo.com', '123456', 'Massachusetts', 'sales', 0, now()),('Tom', 'Spears', 'tom@yahoo.com', '123456', 'Massachusetts', 'sales', 0, now()); Select SELECT * FROM users; SELECT first_name, last_name FROM users; Where Clause SELECT * FROM users WHERE location='Massachusetts'; SELECT * FROM users WHERE location='Massachusetts' AND dept='sales'; SELECT * FROM users WHERE is_admin = 1; SELECT * FROM users WHERE is_admin &gt; 0; Delete Row DELETE FROM users WHERE id = 6; Update Row UPDATE users SET email = 'freddy@gmail.com' WHERE id = 2; Add New Column ALTER TABLE users ADD age VARCHAR(3); Modify Column ALTER TABLE users MODIFY COLUMN age INT(3); Order By (Sort) SELECT * FROM users ORDER BY last_name ASC; SELECT * FROM users ORDER BY last_name DESC; Concatenate Columns SELECT CONCAT(first_name, ' ', last_name) AS 'Name', dept FROM users; Select Distinct Rows SELECT DISTINCT location FROM users; Between (Select Range) SELECT * FROM users WHERE age BETWEEN 20 AND 25; Like (Searching) SELECT * FROM users WHERE dept LIKE 'd%'; SELECT * FROM users WHERE dept LIKE 'dev%'; SELECT * FROM users WHERE dept LIKE '%t'; SELECT * FROM users WHERE dept LIKE '%e%'; Not Like SELECT * FROM users WHERE dept NOT LIKE 'd%'; IN SELECT * FROM users WHERE dept IN ('design', 'sales'); Create &amp; Remove Index CREATE INDEX LIndex On users(location); DROP INDEX LIndex ON users; New Table With Foreign Key (Posts) CREATE TABLE posts( id INT AUTO_INCREMENT, user_id INT, title VARCHAR(100), body TEXT, publish_date DATETIME DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY(id), FOREIGN KEY (user_id) REFERENCES users(id) ); Add Data to Posts Table INSERT INTO posts(user_id, title, body) VALUES (1, 'Post One', 'This is post one'),(3, 'Post Two', 'This is post two'),(1, 'Post Three', 'This is post three'),(2, 'Post Four', 'This is post four'),(5, 'Post Five', 'This is post five'),(4, 'Post Six', 'This is post six'),(2, 'Post Seven', 'This is post seven'),(1, 'Post Eight', 'This is post eight'),(3, 'Post Nine', 'This is post none'),(4, 'Post Ten', 'This is post ten'); INNER JOIN SELECT users.first_name, users.last_name, posts.title, posts.publish_date FROM users INNER JOIN posts ON users.id = posts.user_id ORDER BY posts.title; New Table With 2 Foriegn Keys CREATE TABLE comments( id INT AUTO_INCREMENT, post_id INT, user_id INT, body TEXT, publish_date DATETIME DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY(id), FOREIGN KEY(user_id) references users(id), FOREIGN KEY(post_id) references posts(id) ); Add Data to Comments Table INSERT INTO comments(post_id, user_id, body) VALUES (1, 3, 'This is comment one'),(2, 1, 'This is comment two'),(5, 3, 'This is comment three'),(2, 4, 'This is comment four'),(1, 2, 'This is comment five'),(3, 1, 'This is comment six'),(3, 2, 'This is comment six'),(5, 4, 'This is comment seven'),(2, 3, 'This is comment seven'); Left Join SELECT comments.body, posts.title FROM comments LEFT JOIN posts ON posts.id = comments.post_id ORDER BY posts.title; Join Multiple Tables SELECT comments.body, posts.title, users.first_name, users.last_name FROM comments INNER JOIN posts on posts.id = comments.post_id INNER JOIN users on users.id = comments.user_id ORDER BY posts.title; Aggregate Functions SELECT COUNT(id) FROM users; SELECT MAX(age) FROM users; SELECT MIN(age) FROM users; SELECT SUM(age) FROM users; SELECT UCASE(first_name), LCASE(last_name) FROM users; Group By SELECT age, COUNT(age) FROM users GROUP BY age; SELECT age, COUNT(age) FROM users WHERE age &gt; 20 GROUP BY age; SELECT age, COUNT(age) FROM users GROUP BY age HAVING count(age) &gt;=2; ","link":"https://sueying.github.io/post/mysql-cheatsheet/"},{"title":"常用标点符号的英文名字","content":" 中文 标点符号 英文 句号 . full stop 逗号 , comma 冒号 : colon 分号 ; semicolon 问号 ? question mark 感叹号 ! exclamation mark 破折号 -- dash 省略号 ... dots / ellipsis 左花括号 { open curly / open brace 左方括号 [ open bracket 左圆括号 ( open paren 右圆括号 ) close paren 引号 &quot; quotation marks 井号 # pound 波浪符 ~ tilde 星号，乘号，星 * asterisk, multiply, star 斜杠，除号 / slash, divide 双斜线，注释符 // slash-slash, comment 双线号 || parallel 反斜线转义符 \\ backslash ","link":"https://sueying.github.io/post/punctuation_mark/"},{"title":"Yoon Kim: Convolutional Neural Networks for Sentence Classification, 2014.","content":" 基于预训word2vec，使用CNN做短文本分类。 这篇paper在语句分类方面很有名，模型简单效果好。 论文 TextCNN详读 TextCNN Introduction 两路channel：一路是non-static，一路是static。 non-static就是词向量随着模型训练变化。这样的好处是词向量可以根据数据集做适当调整 static就是直接使用word2vec训练好的词向量。 Model 如图：左边输入，右边输出。 左边是一个 n * k矩阵，表示一句话中的 n 个词（不足n就padding），每个词由一个 k 维向量表示：x1:n=x1⊕x2⊕…⊕xn\\mathbf{x}_{1: n}=\\mathbf{x}_{1} \\oplus \\mathbf{x}_{2} \\oplus \\ldots \\oplus \\mathbf{x}_{n}x1:n​=x1​⊕x2​⊕…⊕xn​ 然后设置一个滑窗的长度 h，用这个滑窗滑过整个矩阵 {x1:h,x2:h+1,…,xn−h+1:n}\\left\\{x_{1}: h, x_{2}: h+1, \\ldots, x_{n-h+1: n}\\right\\}{x1​:h,x2​:h+1,…,xn−h+1:n​} ，Xi:i+j\\mathbf{X}_{i: i+j}Xi:i+j​ 代表第 i ， i+1， ..., i+j个词的聚合。 用滑窗提取出 feature：ci=f(w⋅xi:i+h−1+b)c_{i}=f\\left(\\mathbf{w} \\cdot \\mathbf{x}_{i: i+h-1}+b\\right)ci​=f(w⋅xi:i+h−1​+b)，组成一个 feature map：c=[c1,c2,…,cn−h+1]\\mathbf{c}=\\left[c_{1}, c_{2}, \\ldots, c_{n-h+1}\\right]c=[c1​,c2​,…,cn−h+1​]，我们可以通过改变 h 的大小，生成很多 feature map。 然后对于每个feature map，采取选出这个向量中的最大值（max-over-time pooling，意在找到最重要的特征）：c^=max⁡{c}\\hat{c}=\\max \\{\\mathbf{c}\\}c^=max{c}。同时也解决了每个feature map不等长，统一了维度的问题。 最后再把 z=[c^1,…,c^m]\\mathbf{z}=\\left[\\hat{c}_{1}, \\ldots, \\hat{c}_{m}\\right]z=[c^1​,…,c^m​] 传递到全连接层，做softmax，输出的就是对于不同的 label 的概率分布。 P.S. 如果数据集相对较小，很容易就会发生过拟合现象，所以全连接层用dropout来减少过拟合现象。不是用 y=w⋅z+by=\\mathbf{w} \\cdot \\mathbf{z}+by=w⋅z+b ，而是用 y=w⋅(z∘r)+by=\\mathbf{w} \\cdot(\\mathbf{z} \\circ \\mathbf{r})+by=w⋅(z∘r)+b。其中向量 r 中随机选一步维度置零，剩下的置一，作用是dropout，将pooling之后的结果随机mask一部分值 。 Model Variations 用了四种CNN，比较它们的效果： CNN-rand : 所有的词向量都随机初始化，并且作为模型参数进行训练。 CNN-static : 即用word2vec预训练好的向量（Google News），在训练过程中不更新词向量，句中若有单词不在预训练好的词典中，则用随机数来代替。只学习其他参数。 CNN-non-static : 根据不同的分类任务，进行相应的词向量预训练。pretrained vector在训练过程中要被fine-tuned。 CNN-multichannel : two sets of word vectors. 初始化时两个channel都直接赋值word2vec得出的结果，每个filter也会分别applied到两个channel，但是训练过程中只有一个channel会进行BP误差反向传播，只更新一组词向量。 模型中除了这些参数改变，其他参数相同。 Regularization 在全连接层中用到了dropout去除过拟合，dropout率为0.5，l2正则为0.3，最小batch为50。 Result 在七组数据集上进行了对比实验，CNN-rand效果并不是特别显著，CNN-static效果突然上升，证明了单层的CNN在文本分类任务中的有效性，同时也说明了用无监督学习来的词向量（如word2vec) 对于很多nlp任务都非常有意义。 static 与 non-static：static模型中word2vec预训练出的词向量会把good和bad当做相似的词，在sentiment classification任务中将会导致错误的结果，而non-static模型因为用了当前task dataset作为训练数据，不会存在这样的问题。 单通道与多通道：本来希望多通道的效果更好，多通道能够防止过拟合，实际上两种方法的效果在特定的数据集中效果有优有劣。 ","link":"https://sueying.github.io/post/yoon-kim-convolutional-neural-networks-for-sentence-classification-2014/"},{"title":"Hello Gridea","content":"👏 欢迎使用 Gridea ！ ✍️ Gridea 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ... Github Gridea 主页 示例网站 特性👇 📝 你可以使用最酷的 Markdown 语法，进行快速创作 🌉 你可以给文章配上精美的封面图和在文章任意位置插入图片 🏷️ 你可以对文章进行标签分组 📋 你可以自定义菜单，甚至可以创建外部链接菜单 💻 你可以在 Windows，MacOS 或 Linux 设备上使用此客户端 🌎 你可以使用 𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌 或 Coding Pages 向世界展示，未来将支持更多平台 💬 你可以进行简单的配置，接入 Gitalk 或 DisqusJS 评论系统 🇬🇧 你可以使用中文简体或英语 🌁 你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力 🖥 你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步 🌱 当然 Gridea 还很年轻，有很多不足，但请相信，它会不停向前 🏃 未来，它一定会成为你离不开的伙伴 尽情发挥你的才华吧！ 😘 Enjoy~ ","link":"https://sueying.github.io/post/hello-gridea/"}]}